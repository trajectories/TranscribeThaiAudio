{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPgb5AgJtdepw60Io22V8fr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trajectories/TranscribeThaiAudio/blob/main/transcribe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhRR9WvRMiZi"
      },
      "outputs": [],
      "source": [
        "!pip install pyannote.audio\n",
        "!pip install pytube\n",
        "!pip install soundfile\n",
        "!pip install pydub ffmpeg\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install librosa\n",
        "!sudo apt install ffmpeg\n",
        "!pip install torchaudio ipywebrtc notebook\n",
        "!pip install -q gradio\n",
        "!jupyter nbextension enable --py widgetsnbextension"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube, Playlist\n",
        "from pytube.exceptions import AgeRestrictedError\n",
        "import re\n",
        "\n",
        "# Playlist : Ai content for training By P'Leng\n",
        "p = Playlist('https://youtube.com/playlist?list=PLVdvCctuDZk6Clj_2P_ZIQ18k5MRt-vso&si=lIA3pBxJ3cOHH35e')\n",
        "path_to_save = \"videos/\"\n",
        "\n",
        "for i, video in enumerate(p.videos):\n",
        "  try:\n",
        "    filename = f\"{i}-{video.title}\"\n",
        "    filename = re.sub('[^A-Za-z0-9 \\u0E00-\\u0E7F]+', '', video.title).replace(' ', '_') + '.wav'\n",
        "    print(filename)\n",
        "    video_stream = video.streams.filter(only_audio=True).first()\n",
        "    video_stream.download(filename=filename, output_path=path_to_save)\n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "ePh4ABbbNc5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f931bce-84fb-4ead-9ee9-3039af3c4575"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The_Forestiasโครงการต้นแบบของความยั่งยืนและสภาพแวดล้อมที่มีสุขภาพดี__Suthichai_live_162564.wav\n",
            "Suthichai_Podcast_ความร่วมมือสร้างสรรค์ความบันเทิงระดับโลกสู่_The_Forestias.wav\n",
            "The_Forestias_ระบบนิเวศที่สมบูรณ์แบบกลางใจเมือง_ตอน2.wav\n",
            "Suthichai_Podcast_The_Forestias_กับ_นวัตกรรมการออกแบบเพื่อความยั่งยืน.wav\n",
            "รีวิวโครงการเดอะ_ฟอเรสเทียส์_The_Forestias_by_MQDC.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "\n",
        "folder_path = \"videos\"\n",
        "folder_audio_path = \"audios\"\n",
        "\n",
        "all_files = os.listdir(folder_path)\n",
        "audio_formats = [\"mp3\", \"wav\"]\n",
        "\n",
        "for file in all_files:\n",
        "  file_extension = file.split('.')[-1]\n",
        "\n",
        "  if file_extension in audio_formats:\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "    try:\n",
        "      audio = AudioSegment.from_file(file_path)\n",
        "      audio = audio.set_frame_rate(16000).set_channels(1)\n",
        "      output_path = os.path.join(folder_audio_path, os.path.splitext(file)[0] + \".wav\")\n",
        "      audio.export(output_path, format=\"wav\")\n",
        "    except Exception as e:\n",
        "      print(f\"Failed to convert {file}. Error: {e}\")"
      ],
      "metadata": {
        "id": "SVncYf1hQxcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import re\n",
        "from pydub import AudioSegment\n",
        "from pyannote.audio import Pipeline\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "folder_path = \"audios\"\n",
        "audio_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".wav\")]\n",
        "\n",
        "# # Sort the audio files in ascending order\n",
        "# audio_files.sort()\n",
        "\n",
        "# # Select only the first 33 files\n",
        "# audio_files = audio_files[:33]\n",
        "\n",
        "AUTH_TOKEN_READ = \"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\"\n",
        "diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=AUTH_TOKEN_READ)\n",
        "diarization_pipeline.to(torch.device(\"cuda\"))\n",
        "\n",
        "MODEL_NAME = \"biodatlab/whisper-th-medium-combined\"\n",
        "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "asr_pipeline = pipeline(task=\"automatic-speech-recognition\", model=MODEL_NAME, chunk_length_s=30, device=device)\n",
        "\n",
        "i = 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzWozo-XN1e6",
        "outputId": "c1ede9d2-356e-4705-c68b-3a41b1426a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.0.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.0.1+cu118. Bad things might happen unless you revert torch to 1.x.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for audio_path in audio_files:\n",
        "  print(audio_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFMwoCQVwXgW",
        "outputId": "f2d70f77-14f5-4411-b5ff-982375792875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audios/โครงการเมืองแห่งแรกในโลกที่ออกแบบทุกมิติเพื่อการใช้ชีวิต.wav\n",
            "audios/รวมพลังคืนผืนป่าสู่แผ่นดิน_THE_FORESTIAS_by_MQDC.wav\n",
            "audios/การสร้างป่า_30_ไร่_ในโครงการ_The_Forestias_บางนา_ที่เป็นมากกว่าที่อยู่อาศัย_l_Cloud_Documentary.wav\n",
            "audios/รีวิวโครงการเดอะ_ฟอเรสเทียส์_The_Forestias_by_MQDC.wav\n",
            "audios/วิธีคิดกลับหัว_เอาความยั่งยืนเป็นตัวตั้งแบบ_THE_FORESTIAS__The_Secret_Sauce_EP203.wav\n",
            "audios/The_Forestias_x_Kittiphun_Ouiyamaphu_l_Sustainable_Happiness.wav\n",
            "audios/The_Forestias_StoryBeyondโครงการเมืองต้นแบบของที่อยู่อาศัยที่จะเชื่อมโลกจริงและโลกเสมือนเข้าด้วยกัน.wav\n",
            "audios/กิตติพันธุ์_อุยยามะพันธุ์__ผู้สร้าง_The_Forestias_ให้เป็นเมืองต้นแบบเพื่อสุขภาพ__The_People_Story.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for audio_path in audio_files:\n",
        "  if(audio_path == \"audios/The_Forestias_x_Kittiphun_Ouiyamaphu_l_Sustainable_Happiness.wav\"):\n",
        "    # Load and convert audio to WAV format\n",
        "    audio = AudioSegment.from_file(audio_path)\n",
        "    temp_output_path = \"temp_output.wav\"\n",
        "    audio.export(temp_output_path, format=\"wav\")\n",
        "\n",
        "    # Diarization\n",
        "    diarization = diarization_pipeline(temp_output_path)\n",
        "\n",
        "    audio_files = []\n",
        "\n",
        "    for idx, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True)):\n",
        "        start_time = int(turn.start * 1000)\n",
        "        end_time = int(turn.end * 1000)\n",
        "\n",
        "        speaker_clip = audio[start_time:end_time]\n",
        "\n",
        "        filename = f\"audio_segments/speaker_{speaker}_segment_{idx}.wav\"\n",
        "        speaker_clip.export(filename, format=\"wav\")\n",
        "        print(f\"Exported {filename}\")\n",
        "\n",
        "        audio_files.append(filename)\n",
        "\n",
        "    print(audio_files)\n",
        "\n",
        "    audio_batch = []\n",
        "\n",
        "    for audio_file in audio_files:\n",
        "        audio_batch.append(audio_file)\n",
        "\n",
        "    texts = asr_pipeline(audio_batch, generate_kwargs={\"language\": \"<|th|>\", \"task\": \"transcribe\"}, batch_size=16)\n",
        "\n",
        "    results = []\n",
        "    output_text = \"\"\n",
        "\n",
        "    for idx, text_result in enumerate(texts):\n",
        "        audio_file = audio_files[idx]\n",
        "        speaker_match = re.search(r'speaker_(SPEAKER_\\d+)', audio_file)\n",
        "        if speaker_match:\n",
        "            speaker = speaker_match.group(1)\n",
        "            results.append((speaker, text_result['text']))\n",
        "            output_text += f\"{speaker} : \\\"{text_result['text']}\\\"\\n\"\n",
        "\n",
        "    # Save results to Excel in the \"results\" folder\n",
        "    text_folder = \"text\"\n",
        "    excel_folder = \"excel\"\n",
        "    os.makedirs(text_folder, exist_ok=True)\n",
        "    os.makedirs(excel_folder, exist_ok=True)\n",
        "    base_name = os.path.basename(audio_path)\n",
        "    name_without_extension, _ = os.path.splitext(base_name)\n",
        "    filename_xlsx = os.path.join(excel_folder, f\"{name_without_extension}.xlsx\")\n",
        "    filename_text = os.path.join(text_folder, f\"{name_without_extension}.txt\")\n",
        "    df = pd.DataFrame(results, columns=[\"Speaker\", \"Text\"])\n",
        "    df.to_excel(filename_xlsx, index=False)\n",
        "\n",
        "    with open(filename_text, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(output_text)\n",
        "\n",
        "    # Delete temporary files\n",
        "    os.remove(temp_output_path)\n",
        "    for audio_file in audio_files:\n",
        "        os.remove(audio_file)\n",
        "    print(\"No. \", i)\n",
        "    i += 1\n",
        "    # Release GPU memory\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDiAvqPNPk3w",
        "outputId": "9bc42bc9-c491-45a0-b37f-964429c36247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported audio_segments/speaker_SPEAKER_00_segment_0.wav\n",
            "Exported audio_segments/speaker_SPEAKER_00_segment_1.wav\n",
            "Exported audio_segments/speaker_SPEAKER_00_segment_2.wav\n",
            "Exported audio_segments/speaker_SPEAKER_00_segment_3.wav\n",
            "Exported audio_segments/speaker_SPEAKER_00_segment_4.wav\n",
            "['audio_segments/speaker_SPEAKER_00_segment_0.wav', 'audio_segments/speaker_SPEAKER_00_segment_1.wav', 'audio_segments/speaker_SPEAKER_00_segment_2.wav', 'audio_segments/speaker_SPEAKER_00_segment_3.wav', 'audio_segments/speaker_SPEAKER_00_segment_4.wav']\n",
            "No.  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "SNJcQKaix7Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r excel.zip excel/\n",
        "!zip -r text.zip text/"
      ],
      "metadata": {
        "id": "jymYHS4VeNkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install ffmpeg\n",
        "!pip install datasets transformers pydub pandas attacut ssg"
      ],
      "metadata": {
        "id": "xsOP278pg05b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/biodatlab/thonburian-whisper.git"
      ],
      "metadata": {
        "id": "Tc0fXmgxg3yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyannote.audio\n",
        "!pip install pytube\n",
        "!pip install soundfile\n",
        "!pip install pydub ffmpeg\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install librosa\n",
        "!sudo apt install ffmpeg\n",
        "!pip install torchaudio ipywebrtc notebook\n",
        "!pip install -q gradio\n",
        "!jupyter nbextension enable --py widgetsnbextension"
      ],
      "metadata": {
        "id": "AdUIZdvbg6fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".wav\")]\n",
        "for audio_path in audio_files:\n",
        "  output_file_path = f\"/content/srt/{os.path.splitext(os.path.basename(audio_path))[0]}.srt\"\n",
        "  print(output_file_path)"
      ],
      "metadata": {
        "id": "Kjmg2JSWhde0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "3EKQE2DKikCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/thonburian-whisper/longform_transcription"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acBm56T0irW6",
        "outputId": "a40ebb52-e496-4619-aba1-8564baa05769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/thonburian-whisper/longform_transcription\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"audio_segments\"\n",
        "audio_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".wav\")]\n",
        "for audio_file in audio_files:\n",
        "  os.remove(audio_file)\n"
      ],
      "metadata": {
        "id": "67F8jLvYsSk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"temp_directory_for_chunks\"\n",
        "audio_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".json\")]\n",
        "for audio_file in audio_files:\n",
        "  os.remove(audio_file)"
      ],
      "metadata": {
        "id": "NxxA6XjOtJvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "audio_path = \"/content/audios/รวมพลังคืนผืนป่าสู่แผ่นดิน_THE_FORESTIAS_by_MQDC.wav\"\n",
        "output_csv_file_path = f\"csv/{os.path.splitext(os.path.basename(audio_path))[0]}.csv\"\n",
        "output_srt_file_path = f\"srt/{os.path.splitext(os.path.basename(audio_path))[0]}.srt\"\n",
        "!python main.py --input_file {audio_path} --output_csv_file {output_csv_file_path} --output_srt_file {output_srt_file_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvM98REMkdNw",
        "outputId": "78d3a74b-7cd9-4f59-f868-7ecffcba57cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-19 06:53:14.013848: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-10-19:06:53:22 INFO     [pytorch_lightning.utilities.migration.utils:153] Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.0.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.0.1+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Exported audio_segments/speaker_SPEAKER_02_segment_0.wav\n",
            "Exported audio_segments/speaker_SPEAKER_00_segment_1.wav\n",
            "Exported audio_segments/speaker_SPEAKER_01_segment_2.wav\n",
            "Exported audio_segments/speaker_SPEAKER_01_segment_3.wav\n",
            "Exported audio_segments/speaker_SPEAKER_01_segment_4.wav\n",
            "Exported audio_segments/speaker_SPEAKER_01_segment_5.wav\n",
            "Exported audio_segments/speaker_SPEAKER_01_segment_6.wav\n",
            "Exported audio_segments/speaker_SPEAKER_00_segment_7.wav\n",
            "Exported audio_segments/speaker_SPEAKER_01_segment_8.wav\n",
            "Exported audio_segments/speaker_SPEAKER_01_segment_9.wav\n",
            "Exported audio_segments/speaker_SPEAKER_00_segment_10.wav\n",
            "Exported audio_segments/speaker_SPEAKER_01_segment_11.wav\n",
            "Exported audio_segments/speaker_SPEAKER_01_segment_12.wav\n",
            "Exported audio_segments/speaker_SPEAKER_02_segment_13.wav\n",
            "Exported audio_segments/speaker_SPEAKER_02_segment_14.wav\n",
            "Exported audio_segments/speaker_SPEAKER_02_segment_15.wav\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for audio_path in audio_files:\n",
        "    output_csv_file_path = f\"/content/csv/{os.path.splitext(os.path.basename(audio_path))[0]}.csv\"\n",
        "    output_srt_file_path = f\"/content/srt/{os.path.splitext(os.path.basename(audio_path))[0]}.srt\"\n",
        "    !python main.py --input_file {audio_path} --output_csv_file {output_csv_file_path} --output_srt_file {output_srt_file_path}\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "6meZT00Nijkk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}